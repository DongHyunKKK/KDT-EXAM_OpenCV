{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\Torch_PY38\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] 지정된 프로시저를 찾을 수 없습니다'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# 모듈 로딩\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 데이터 로딩\n",
    "file = '../DATA/IMAGE/naeun/naeun1.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('JPEG', 625, 500, 'RGB', PIL.JpegImagePlugin.JpegImageFile)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### PIL\n",
    "img = Image.open(file)\n",
    "\n",
    "img.format, img.height, img.width, img.mode, type(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.Size([3, 625, 500]) 3\n",
      "tensor([[0.9647, 0.9647, 0.9647,  ..., 0.9059, 0.9059, 0.9059],\n",
      "        [0.9647, 0.9647, 0.9647,  ..., 0.9059, 0.9059, 0.9059],\n",
      "        [0.9647, 0.9647, 0.9647,  ..., 0.9059, 0.9059, 0.9059],\n",
      "        ...,\n",
      "        [0.8863, 0.8863, 0.8902,  ..., 0.8902, 0.8902, 0.8902],\n",
      "        [0.8863, 0.8863, 0.8902,  ..., 0.8902, 0.8902, 0.8902],\n",
      "        [0.8902, 0.8902, 0.8902,  ..., 0.8902, 0.8902, 0.8902]])\n"
     ]
    }
   ],
   "source": [
    "# 사용법\n",
    "\n",
    "# (1) 인스턴스 생성\n",
    "to_tensor = transforms.ToTensor()\n",
    "\n",
    "# (2) 인스턴스 변수 사용\n",
    "imgTS = to_tensor(img)\n",
    "\n",
    "# (3) 변환된 이미지 텐서 확인\n",
    "print(type(imgTS), imgTS.shape, imgTS.ndim)\n",
    "print(imgTS[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (625, 500, 3) 3\n",
      "<class 'numpy.ndarray'> (625, 500, 3) 3\n",
      "<class 'torch.Tensor'> torch.Size([3, 625, 500]) 3 tensor([[0.9922, 0.9922, 0.9922,  ..., 0.9451, 0.9451, 0.9451],\n",
      "        [0.9922, 0.9922, 0.9922,  ..., 0.9451, 0.9451, 0.9451],\n",
      "        [0.9922, 0.9922, 0.9922,  ..., 0.9451, 0.9451, 0.9451],\n",
      "        ...,\n",
      "        [0.9216, 0.9216, 0.9255,  ..., 0.9294, 0.9294, 0.9294],\n",
      "        [0.9216, 0.9216, 0.9255,  ..., 0.9294, 0.9294, 0.9294],\n",
      "        [0.9255, 0.9255, 0.9255,  ..., 0.9294, 0.9294, 0.9294]])\n",
      "<class 'torch.Tensor'> torch.Size([3, 625, 500]) 3 tensor([[0.9647, 0.9647, 0.9647,  ..., 0.9059, 0.9059, 0.9059],\n",
      "        [0.9647, 0.9647, 0.9647,  ..., 0.9059, 0.9059, 0.9059],\n",
      "        [0.9647, 0.9647, 0.9647,  ..., 0.9059, 0.9059, 0.9059],\n",
      "        ...,\n",
      "        [0.8863, 0.8863, 0.8902,  ..., 0.8902, 0.8902, 0.8902],\n",
      "        [0.8863, 0.8863, 0.8902,  ..., 0.8902, 0.8902, 0.8902],\n",
      "        [0.8902, 0.8902, 0.8902,  ..., 0.8902, 0.8902, 0.8902]])\n"
     ]
    }
   ],
   "source": [
    "### openCV\n",
    "import cv2\n",
    "\n",
    "img = cv2.imread(file)\n",
    "img2 = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # RGB로 변환\n",
    "\n",
    "print(type(img), img.shape, img.ndim)\n",
    "print(type(img2), img2.shape, img2.ndim)\n",
    "\n",
    "# 텐서화\n",
    "imgTS2 = to_tensor(img)\n",
    "imgTS3 = to_tensor(img2)\n",
    "\n",
    "print(type(imgTS2), imgTS2.shape, imgTS2.ndim, imgTS2[0])\n",
    "print(type(imgTS3), imgTS3.shape, imgTS3.ndim, imgTS3[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing = transforms.Compose(\n",
    "    [transforms.Resize(size = (50, 50)),\n",
    "     transforms.RandomHorizontalFlip(p = 0.5),\n",
    "     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\Torch_PY38\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9059, 0.9020, 0.8980,  ..., 0.9490, 0.9569, 0.9627],\n",
       "         [0.9059, 0.9000, 0.8980,  ..., 0.9451, 0.9569, 0.9608],\n",
       "         [0.9020, 0.8980, 0.8941,  ..., 0.9451, 0.9569, 0.9608],\n",
       "         ...,\n",
       "         [0.8863, 0.8784, 0.8770,  ..., 0.8824, 0.8863, 0.8902],\n",
       "         [0.8863, 0.8799, 0.8809,  ..., 0.8824, 0.8902, 0.8941],\n",
       "         [0.8902, 0.8863, 0.8931,  ..., 0.8824, 0.8863, 0.8902]],\n",
       "\n",
       "        [[0.9373, 0.9333, 0.9294,  ..., 0.9647, 0.9725, 0.9784],\n",
       "         [0.9373, 0.9314, 0.9294,  ..., 0.9647, 0.9725, 0.9765],\n",
       "         [0.9333, 0.9294, 0.9255,  ..., 0.9647, 0.9725, 0.9765],\n",
       "         ...,\n",
       "         [0.9176, 0.9098, 0.8966,  ..., 0.9020, 0.9059, 0.9098],\n",
       "         [0.9176, 0.9113, 0.9005,  ..., 0.9020, 0.9098, 0.9137],\n",
       "         [0.9216, 0.9176, 0.9127,  ..., 0.9020, 0.9059, 0.9098]],\n",
       "\n",
       "        [[0.9451, 0.9412, 0.9373,  ..., 0.9765, 0.9843, 0.9902],\n",
       "         [0.9451, 0.9392, 0.9373,  ..., 0.9804, 0.9843, 0.9882],\n",
       "         [0.9412, 0.9373, 0.9333,  ..., 0.9804, 0.9843, 0.9882],\n",
       "         ...,\n",
       "         [0.9294, 0.9216, 0.9201,  ..., 0.9176, 0.9216, 0.9255],\n",
       "         [0.9294, 0.9230, 0.9240,  ..., 0.9176, 0.9255, 0.9294],\n",
       "         [0.9333, 0.9294, 0.9284,  ..., 0.9176, 0.9216, 0.9255]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ndarray는 안되고 tensor는 가능\n",
    "preprocessing(imgTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이미지 데이터 셋 <hr>\n",
    "- torchvision.ImageFolder 클래스 사용\n",
    "    * 이미지 데이터 라벨링\n",
    "    * 이미지 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing = transforms.Compose(\n",
    "    [transforms.Resize(size = (50, 50)),\n",
    "     transforms.RandomHorizontalFlip(p = 0.5),\n",
    "     transforms.ToTensor()\n",
    "     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_root = '../DATA/IMAGE'\n",
    "imgDS = ImageFolder(root = img_root,\n",
    "            transform = preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['naeun', 'yeeun'],\n",
       " {'naeun': 0, 'yeeun': 1},\n",
       " [('../DATA/IMAGE\\\\naeun\\\\naeun1.jpg', 0),\n",
       "  ('../DATA/IMAGE\\\\naeun\\\\naeun2.jpg', 0),\n",
       "  ('../DATA/IMAGE\\\\yeeun\\\\yeeun1.jpg', 1),\n",
       "  ('../DATA/IMAGE\\\\yeeun\\\\yeeun2.png', 1)])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 폴더명이 분류 클래스\n",
    "imgDS.classes, imgDS.class_to_idx, imgDS.imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 50, 50]) torch.Size([1]) tensor([0])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "imgDL = DataLoader(dataset = imgDS)\n",
    "\n",
    "for (img, label) in imgDL:\n",
    "    print(img.shape, label.shape, label)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch_PY38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
